{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakamenori/txt2imghd-colab/blob/master/txt2imghd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apf9g8sCLBZw",
        "outputId": "aab5dbce-54f3-48c3-cb3e-fa1c15294040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-d17781ce-e905-a2a0-64fe-8fd1b62eb0e2)\n",
            "\n",
            "Gen RAM Free: 25.7 GB  | Proc size: 95.6 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ]
        }
      ],
      "source": [
        "#@title Check GPU\n",
        "\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install -qq gputil psutil humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "clear_output()\n",
        "!nvidia-smi -L\n",
        "print()\n",
        "printm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjpSIU4soPax"
      },
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI5Nf8CmEHeG"
      },
      "source": [
        "## Introduction\n",
        "This notebook is basically a Colab version of [txt2imghd](https://github.com/jquesnelle/txt2imghd) and also NSFW disabled.  \n",
        "I take some codes from the original repo and made it work on Colab. It's easy for people who has no coding background to use it.\n",
        "\n",
        "`txt2imghd` generates high-resolution images using txt2img and img2img.\n",
        "> txt2imghd is a port of the GOBIG mode from progrockdiffusion applied to Stable Diffusion, with Real-ESRGAN as the upscaler.  \n",
        "> It creates detailed, higher-resolution images by first generating an image from a prompt, upscaling it, and then running img2img on smaller pieces of the upscaled image, and blending the result back into the original image.  \n",
        "\n",
        "(quoted from [original repo](https://github.com/jquesnelle/txt2imghd))\n",
        "\n",
        "## How it works\n",
        "1. Generate an image from a prompt using txt2img. Or load a image file.\n",
        "2. Scale up the image using Real-ESRGAN.\n",
        "3. Run img2img on smaller pieces of the up-scaled image.\n",
        "4. Blend the result back into the upscaled image.\n",
        "\n",
        "### Detailed explanation\n",
        "#### Step.1 Generate an image from a prompt using txt2img\n",
        "> Let's say you generate an image in pixel size of 512x512 using txt2img.\n",
        "![Original image](https://github.com/wakamenori/txt2imghd-colab/blob/master/gallery/Original.png?raw=true)\n",
        "\n",
        "#### Step.2 Scale up the image using Real-ESRGAN\n",
        "> Scale up the image to 1024x1024.\n",
        "![up-scaled](https://github.com/wakamenori/txt2imghd-colab/blob/master/gallery/up-scaled_2x.png?raw=true)\n",
        "\n",
        "#### Step.3 Run img2img on smaller pieces of the up-scaled image\n",
        "> Chop up-scaled image into slices in pixel size of original image.  \n",
        "In this example, up-scaled image will be chopped into 9 images in pixel size of 512x512.  \n",
        "Then run img2img on every small images with the same prompt for original image.  \n",
        "This process generates more detailed and cleaner images compared to just up scaling the image\n",
        "![img2img-1](https://github.com/wakamenori/txt2imghd-colab/blob/master/gallery/slice_img2img.png?raw=true)\n",
        "![img2img-2](https://github.com/wakamenori/txt2imghd-colab/blob/master/gallery/slice_img2img(2).png?raw=true)\n",
        "\n",
        "#### Step4. Blend the result back into the upscaled image.\n",
        "> Finally, blend the slices of images back into the upscaled image.  \n",
        "![txt2imghd](https://github.com/wakamenori/txt2imghd-colab/blob/master/gallery/up-scaled_2x_img2img.png?raw=true)\n",
        "![txt2imghd_diff](https://github.com/wakamenori/txt2imghd-colab/blob/master/gallery/diff.png?raw=true)\n",
        "####  Step.3 and Step.4 again\n",
        "> If you select `SCALEUP_RATIO` 4x or 8x and checked `SCALEUP_STEP_BY_STEP`, Step.3 and Step.4 runs again.  \n",
        "up scale 1024x1024 image to 2048x2048, and chop up-scalled image into 36 pieces then run img2img on every 36 images, then blend them back\n",
        "\n",
        "## `CUDA out of memory` try these\n",
        "Not to run out memory,\n",
        "- Smaller size of the image\n",
        "- Lower `SCALEUP_RATIO`\n",
        "- Uncheck `FP32`\n",
        "- Uncheck `GFPGAN`\n",
        "\n",
        "## Credits\n",
        "Colab\n",
        "- [NSFW Disabled: NOP & WAS's Stable Diffusion Colab v0.35 (1.4 Weights)](https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17#scrollTo=Ucr5_i21xSjv)\n",
        "\n",
        "Reddit\n",
        "- [txt2imghd: Generate high-res images with Stable Diffusion](https://www.reddit.com/r/StableDiffusion/comments/wxm0cf/txt2imghd_generate_highres_images_with_stable/)\n",
        "\n",
        "Github\n",
        "- [txt2imghd](https://github.com/jquesnelle/txt2imghd)\n",
        "- [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN)\n",
        "- [GFPGAN](https://github.com/TencentARC/GFPGAN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ1TOlU1O-HB"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "Executing this section crashes session. **It's intended**.\n",
        "You don't need to re-execute again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgftD7dIuLY3",
        "outputId": "457dece5-5d04-4efb-a6f9-b715b55078d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.7 MB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.1 MB 25.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 365 kB 38.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 41.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 52.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 37.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 52.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 270 kB 43.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 63.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 51.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 51.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 51.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 54.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 46.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 594 kB 59.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 856 kB 60.9 MB/s \n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 6.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#@title Install dependencies\n",
        "!pip install -qq transformers scipy ftfy transformers gradio datasets \"ipywidgets>=7,<8\" diffusers==0.2.4\n",
        "!pip install -qq --ignore-installed Pillow==9.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z3swylMmVa4Y"
      },
      "outputs": [],
      "source": [
        "#@title Restart session\n",
        "#@markdown Executing this cell crashes session. It's intended.<br>You don't need to re-execute cells above.\n",
        "import os\n",
        "os._exit(00)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sKf0QX1Oejc"
      },
      "source": [
        "# Setup pipelines and util functions\n",
        "Read access token for huggingface from a file in Google drive<br>\n",
        "make sure you saved token in text file and uploaded it to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wK82Vdiu3WN",
        "outputId": "bbea7ebb-4980-4afd-9c8e-52844ffcee32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity\n",
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "import inspect\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import PIL\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDIMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    LMSDiscreteScheduler,\n",
        "    PNDMScheduler,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "import gradio as gr\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import LMSDiscreteScheduler\n",
        "import requests\n",
        "import PIL\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "\n",
        "from diffusers.models import AutoencoderKL, UNet2DConditionModel\n",
        "from diffusers.pipeline_utils import DiffusionPipeline\n",
        "from diffusers.schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "\n",
        "\n",
        "def preprocess(image):\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.0 * image - 1.0\n",
        "\n",
        "\n",
        "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[DDIMScheduler, PNDMScheduler],\n",
        "        safety_checker: StableDiffusionSafetyChecker,\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            safety_checker=safety_checker,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        init_image: torch.FloatTensor,\n",
        "        strength: float = 0.8,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        eta: Optional[float] = 0.0,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "    ):\n",
        "\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if strength < 0 or strength > 1:\n",
        "            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        offset = 0\n",
        "        if accepts_offset:\n",
        "            offset = 1\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # encode the init image into latents and scale the latents\n",
        "        init_latents = self.vae.encode(init_image.to(self.device)).sample()\n",
        "        init_latents = 0.18215 * init_latents\n",
        "\n",
        "        # prepare init_latents noise to latents\n",
        "        init_latents = torch.cat([init_latents] * batch_size)\n",
        "\n",
        "        # get the original timestep using init_timestep\n",
        "        init_timestep = int(num_inference_steps * strength) + offset\n",
        "        init_timestep = min(init_timestep, num_inference_steps)\n",
        "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
        "        timesteps = torch.tensor([timesteps] * batch_size, dtype=torch.long, device=self.device)\n",
        "\n",
        "        # add noise to latents using the timesteps\n",
        "        noise = torch.randn(init_latents.shape, generator=generator, device=self.device)\n",
        "        init_latents = self.scheduler.add_noise(init_latents, noise, timesteps)\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        latents = init_latents\n",
        "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
        "        for i, t in enumerate(self.scheduler.timesteps[t_start:]):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents)\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        # run safety checker\n",
        "        # safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "        # image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image, \"nsfw_content_detected\": False}\n",
        "\n",
        "\n",
        "class StableDiffusionPipeline(DiffusionPipeline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
        "        safety_checker: StableDiffusionSafetyChecker,\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            safety_checker=safety_checker,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        height: Optional[int] = 512,\n",
        "        width: Optional[int] = 512,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        eta: Optional[float] = 0.0,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        if \"torch_device\" in kwargs:\n",
        "            device = kwargs.pop(\"torch_device\")\n",
        "            warnings.warn(\n",
        "                \"`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0.\"\n",
        "                \" Consider using `pipe.to(torch_device)` instead.\"\n",
        "            )\n",
        "\n",
        "            # Set device as before (to be removed in 0.3.0)\n",
        "            if device is None:\n",
        "                device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.to(device)\n",
        "\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        # get the intial random noise\n",
        "        latents = torch.randn(\n",
        "            (batch_size, self.unet.in_channels, height // 8, width // 8),\n",
        "            generator=generator,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        if accepts_offset:\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "        # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                sigma = self.scheduler.sigmas[i]\n",
        "                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "            else:\n",
        "                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents)\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        # run safety checker\n",
        "        # safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "        # image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image, \"nsfw_content_detected\": False}\n",
        "\n",
        "\n",
        "PATH_TO_TOKEN_FILE = \"/content/drive/MyDrive/token.txt\"  # @param {type:'string'}\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "with open(PATH_TO_TOKEN_FILE, 'r') as f:\n",
        "    access_tokens = f.read()\n",
        "\n",
        "\n",
        "# lms = LMSDiscreteScheduler(\n",
        "#     beta_start=0.00085,\n",
        "#     beta_end=0.012,\n",
        "#     beta_schedule=\"scaled_linear\",\n",
        "#     num_train_timesteps=1000\n",
        "# )\n",
        "ddim = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",\n",
        "                     clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    scheduler=ddim,\n",
        "    revision=\"fp16\",\n",
        "    use_auth_token=access_tokens\n",
        ").to(\"cuda\")\n",
        "\n",
        "pipeimg = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    scheduler=ddim,\n",
        "    revision=\"fp16\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_auth_token=access_tokens\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "def grid_coords(target, original, overlap):\n",
        "    # generate a list of coordinate tuples for our sections, in order of how they'll be rendered\n",
        "    # target should be the size for the gobig result, original is the size of each chunk being rendered\n",
        "    center = []\n",
        "    target_x, target_y = target\n",
        "    center_x = int(target_x / 2)\n",
        "    center_y = int(target_y / 2)\n",
        "    original_x, original_y = original\n",
        "    x = center_x - int(original_x / 2)\n",
        "    y = center_y - int(original_y / 2)\n",
        "    center.append((x, y))  # center chunk\n",
        "    uy = y  # up\n",
        "    uy_list = []\n",
        "    dy = y  # down\n",
        "    dy_list = []\n",
        "    lx = x  # left\n",
        "    lx_list = []\n",
        "    rx = x  # right\n",
        "    rx_list = []\n",
        "    while uy > 0:  # center row vertical up\n",
        "        uy = uy - original_y + overlap\n",
        "        uy_list.append((lx, uy))\n",
        "    while (dy + original_y) <= target_y:  # center row vertical down\n",
        "        dy = dy + original_y - overlap\n",
        "        dy_list.append((rx, dy))\n",
        "    while lx > 0:\n",
        "        lx = lx - original_x + overlap\n",
        "        lx_list.append((lx, y))\n",
        "        uy = y\n",
        "        while uy > 0:\n",
        "            uy = uy - original_y + overlap\n",
        "            uy_list.append((lx, uy))\n",
        "        dy = y\n",
        "        while (dy + original_y) <= target_y:\n",
        "            dy = dy + original_y - overlap\n",
        "            dy_list.append((lx, dy))\n",
        "    while (rx + original_x) <= target_x:\n",
        "        rx = rx + original_x - overlap\n",
        "        rx_list.append((rx, y))\n",
        "        uy = y\n",
        "        while uy > 0:\n",
        "            uy = uy - original_y + overlap\n",
        "            uy_list.append((rx, uy))\n",
        "        dy = y\n",
        "        while (dy + original_y) <= target_y:\n",
        "            dy = dy + original_y - overlap\n",
        "            dy_list.append((rx, dy))\n",
        "    # calculate a new size that will fill the canvas, which will be optionally used in grid_slice and go_big\n",
        "    last_coordx, last_coordy = dy_list[-1:][0]\n",
        "    render_edgey = last_coordy + original_y  # outer bottom edge of the render canvas\n",
        "    render_edgex = last_coordx + original_x  # outer side edge of the render canvas\n",
        "    scalarx = render_edgex / target_x\n",
        "    scalary = render_edgey / target_y\n",
        "    if scalarx <= scalary:\n",
        "        new_edgex = int(target_x * scalarx)\n",
        "        new_edgey = int(target_y * scalarx)\n",
        "    else:\n",
        "        new_edgex = int(target_x * scalary)\n",
        "        new_edgey = int(target_y * scalary)\n",
        "    # now put all the chunks into one master list of coordinates (essentially reverse of how we calculated them so that the central slices will be on top)\n",
        "    result = []\n",
        "    for coords in dy_list[::-1]:\n",
        "        result.append(coords)\n",
        "    for coords in uy_list[::-1]:\n",
        "        result.append(coords)\n",
        "    for coords in rx_list[::-1]:\n",
        "        result.append(coords)\n",
        "    for coords in lx_list[::-1]:\n",
        "        result.append(coords)\n",
        "    result.append(center[0])\n",
        "    return result, (new_edgex, new_edgey)\n",
        "\n",
        "\n",
        "def grid_slice(source, overlap, og_size, maximize=False):\n",
        "    width, height = og_size  # size of the slices to be rendered\n",
        "    coordinates, new_size = grid_coords(source.size, og_size, overlap)\n",
        "    if maximize == True:\n",
        "        source = source.resize(new_size, get_resampling_mode())  # minor concern that we're resizing twice\n",
        "        # re-do the coordinates with the new canvas size\n",
        "        coordinates, new_size = grid_coords(source.size, og_size, overlap)\n",
        "    # loc_width and loc_height are the center point of the goal size, and we'll start there and work our way out\n",
        "    slices = []\n",
        "    for coordinate in coordinates:\n",
        "        x, y = coordinate\n",
        "        slices.append(((source.crop((x, y, x+width, y+height))), x, y))\n",
        "    global slices_todo\n",
        "    slices_todo = len(slices) - 1\n",
        "    return slices, new_size\n",
        "\n",
        "\n",
        "def convert_pil_img(image):\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.Resampling.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "\n",
        "def addalpha(im, mask):\n",
        "    imr, img, imb, ima = im.split()\n",
        "    mmr, mmg, mmb, mma = mask.split()\n",
        "    # we want the RGB from the original, but the transparency from the mask\n",
        "    im = Image.merge('RGBA', [imr, img, imb, mma])\n",
        "    return (im)\n",
        "\n",
        "\n",
        "def grid_merge(source, slices):\n",
        "    source.convert(\"RGBA\")\n",
        "    for slice, posx, posy in slices:  # go in reverse to get proper stacking\n",
        "        source.alpha_composite(slice, (posx, posy))\n",
        "    return source\n",
        "\n",
        "\n",
        "def display_images(images, figsize=(30, 15), columns=4, seed: Optional[int] = None, titles: Optional[List[str]] = None, label: Optional[str] = None):\n",
        "    plt.figure(\n",
        "        figsize=figsize,\n",
        "        facecolor=\"white\"\n",
        "    )\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    if seed is not None:\n",
        "        plt.suptitle(\"seed is \" + str(seed), y=0.65, x=0.1)\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.xticks(color='w')\n",
        "        plt.yticks(color='w')\n",
        "        if titles is not None:\n",
        "            plt.title(titles[i], fontsize=30)\n",
        "        if label is not None and i == 0:\n",
        "            plt.ylabel(label, fontsize=30)\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "def pil2cv(image):\n",
        "    ''' PIL -> OpenCV '''\n",
        "    new_image = np.array(image, dtype=np.uint8)\n",
        "    if new_image.ndim == 2:\n",
        "        pass\n",
        "    elif new_image.shape[2] == 3:\n",
        "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
        "    elif new_image.shape[2] == 4:\n",
        "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
        "    return new_image\n",
        "\n",
        "\n",
        "def image_diff(before_image, after_image):\n",
        "\n",
        "    # Load images\n",
        "    before = pil2cv(before_image)\n",
        "    after = pil2cv(after_image)\n",
        "\n",
        "    # Convert images to grayscale\n",
        "    before_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY)\n",
        "    after_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Compute SSIM between the two images\n",
        "    (score, diff) = structural_similarity(before_gray, after_gray, full=True)\n",
        "    # print(\"Image Similarity: {:.4f}%\".format(score * 100))\n",
        "\n",
        "    # The diff image contains the actual image differences between the two images\n",
        "    # and is represented as a floating point data type in the range [0,1]\n",
        "    # so we must convert the array to 8-bit unsigned integers in the range\n",
        "    # [0,255] before we can use it with OpenCV\n",
        "    diff = (diff * 255).astype(\"uint8\")\n",
        "    diff_box = cv2.merge([diff, diff, diff])\n",
        "\n",
        "    # Threshold the difference image, followed by finding contours to\n",
        "    # obtain the regions of the two input images that differ\n",
        "    thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "    contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
        "\n",
        "    mask = np.zeros(before.shape, dtype='uint8')\n",
        "    filled_after = after.copy()\n",
        "\n",
        "    for c in contours:\n",
        "        area = cv2.contourArea(c)\n",
        "        if area > 40:\n",
        "            x, y, w, h = cv2.boundingRect(c)\n",
        "            cv2.rectangle(before, (x, y), (x + w, y + h), (36, 255, 12), 2)\n",
        "            cv2.rectangle(after, (x, y), (x + w, y + h), (36, 255, 12), 2)\n",
        "            cv2.rectangle(diff_box, (x, y), (x + w, y + h), (36, 255, 12), 2)\n",
        "            cv2.drawContours(mask, [c], 0, (255, 255, 255), -1)\n",
        "            cv2.drawContours(filled_after, [c], 0, (0, 255, 0), -1)\n",
        "    return filled_after, score\n",
        "\n",
        "\n",
        "def clean_env():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def setup_dirs(root_dir: str):\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    results_dir = os.path.join(root_dir, \"results\")\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def generate_image(prompt: str, width: int = 512, height: int = 512, seed: int = 0, steps: int = 50, guidance_scale: float = 7.5, eta: float = 0.0):\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        if seed == 0:\n",
        "            seed = random.randint(0, sys.maxsize)\n",
        "        print(\"Seed : \", seed)\n",
        "        generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
        "        image = pipe(\n",
        "            prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            generator=generator,\n",
        "            num_inference_steps=steps,\n",
        "            eta=eta,\n",
        "            guidance_scale=guidance_scale\n",
        "        )[\"sample\"][0]\n",
        "    return image\n",
        "\n",
        "\n",
        "def install_gfpgan():\n",
        "    if not os.path.exists('/content/GFPGAN'):\n",
        "        %cd /content\n",
        "        !git clone https://github.com/TencentARC/GFPGAN.git\n",
        "        %cd GFPGAN\n",
        "        # Set up the environment\n",
        "        # Install basicsr - https://github.com/xinntao/BasicSR\n",
        "        # We use BasicSR for both training and inference\n",
        "        !pip install -qq basicsr\n",
        "        # Install facexlib - https://github.com/xinntao/facexlib\n",
        "        # We use face detection and face restoration helper in the facexlib package\n",
        "        !pip install -qq facexlib\n",
        "        # Install other depencencies\n",
        "        !pip install -qq -r requirements.txt\n",
        "        !python setup.py develop\n",
        "        !pip install -qq realesrgan  # used for enhancing the background (non-face) regions\n",
        "        # Download the pre-trained model\n",
        "        # !wget https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth -P experiments/pretrained_models\n",
        "        # Now we use the V1.3 model for the demo\n",
        "        !wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n",
        "        %cd /content/\n",
        "        clear_output()\n",
        "\n",
        "\n",
        "def install_esrgan():\n",
        "    if not os.path.exists('/content/Real-ESRGAN'):\n",
        "        %cd /content\n",
        "        !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "        %cd /content/Real-ESRGAN\n",
        "        # Set up the environment\n",
        "        !pip -qq install basicsr facexlib gfpgan\n",
        "        !pip -qq install -r requirements.txt\n",
        "        !python setup.py develop\n",
        "        # Download the pre-trained model\n",
        "        !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "        !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models\n",
        "        %cd /content\n",
        "    clear_output()\n",
        "\n",
        "\n",
        "def install_swinir():\n",
        "    if not os.path.exists(\"/content/SwinIR\"):\n",
        "        %cd /content\n",
        "        !git clone https://github.com/JingyunLiang/SwinIR.git\n",
        "        !pip install timm\n",
        "        !wget https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth -P / content/SwinIR/models\n",
        "        !wget https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth -P / content/SwinIR/models\n",
        "    clear_output()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhEtbe0p0rhZ"
      },
      "source": [
        "#txt2imghd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ictQdO1IPYE5"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "import os\n",
        "import gc\n",
        "import datetime as dt\n",
        "import sys\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "#@markdown # <u>**【Basics options】**</u>\n",
        "#@markdown You can use a image you already generated as base image.<br>\n",
        "#@markdown leave `BASEIMAGE_PATH` empty if you want to generate a new image.\n",
        "BASEIMAGE_PATH = \"\"  # @param {type:\"string\"}\n",
        "PROMPT = \"anime kyoto animation key by greg rutkowski night, single white hair girl from behind, in abandoned chapel with overgrown flowers and plants\" # @param {type:'string'}\n",
        "HEIGHT = 512  # @param {type:\"slider\", min:256, max:1920, step:64}\n",
        "WIDTH = 512  # @param {type:\"slider\", min:256, max:1920, step:64}\n",
        "STEPS = 50  # @param {type:\"slider\", min:5, max:500, step:5}\n",
        "IMG_NUM = 1  # @param {type:\"slider\", min:1, max:20, step:1}\n",
        "#@markdown #### `SEED` 0 for random\n",
        "SEED = 0  # @param {type:'integer'}\n",
        "if SEED is None:\n",
        "    SEED = 0\n",
        "TXT2IMG_GUIDANCE_SCALE = 7.5  # @param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "TXT2IMG_ETA = 0  # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown #### Absolute path to dir to save images **no space character*<br>\n",
        "#@markdown sub directories will be created<br>\n",
        "#@markdown `ROOT_DIR/original/` : images generated by txt2img<br>\n",
        "#@markdown `ROOT_DIR/restored/restored_imgs/` : images generated by GFPGAN<br>\n",
        "#@markdown `ROOT_DIR/upscaled/` : images generated by Real-ESRGAN and txt2imghd<br>\n",
        "#@markdown `ROOT_DIR/results/` : Final result will be copied here\n",
        "\n",
        "ROOT_DIR = \"/content/drive/MyDrive/SD_PICTURES/txt2imghd\"  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # <u>**【Scaleup options】**</u>\n",
        "# UPSCALE = True #@param{type:'boolean'}\n",
        "SCALEUP_RATIO = \"4x\"  # @param [\"1x\", \"2x\", \"4x\", \"8x\"]\n",
        "#@markdown `SCALEUP_STEP_BY_STEP` If checked, img2img will run every 2x scale up.<br>\n",
        "#@markdown for example, If you set `SCALEUP_RATIO` to `4x`, image will be scaled up to 2x and img2img will run, then scale it up again and img2img will run again.\n",
        "SCALEUP_STEP_BY_STEP = True  # @param{type:'boolean'}\n",
        "ESRGAN_MODEL = \"RealESRGAN_x4plus_anime_6B (Optimized for Anime)\" # @param [\"RealESRGAN_x4plus\", \"RealESRGAN_x4plus_anime_6B (Optimized for Anime)\"]\n",
        "#@markdown `FP_32` Use fp32 precision during inference. Default: fp16 (half precision).\n",
        "FP_32 = False  # @param{type:'boolean'}\n",
        "#@markdown `USE_IMG2IMG` If unchecked, image will be just scaled up without running img2img.\n",
        "USE_IMG2IMG = True  # @param{type:'boolean'}\n",
        "IMG2IMG_STRENGTH = 0.3  # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "IMG2IMG_STEPS = 65  # @param {type:\"slider\", min:5, max:500, step:5}\n",
        "IMG2IMG_GUIDANCE_SCALE = 7.5  # @param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "IMG2IMG_ETA = 0  # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown ---\n",
        "#@markdown ## *Restore **realistic** face with [GFPGAN](https://github.com/TencentARC/GFPGAN)*\n",
        "GFPGAN_BEFORE_SCALEUP = False  # @param{type:'boolean'}\n",
        "GFPGAN_AFTER_SCALEUP = False  # @param{type:'boolean'}\n",
        "\n",
        "\n",
        "def gfpgan_interface(input_path: str, output_dir: str) -> str:\n",
        "    %cd /content/GFPGAN\n",
        "    !python /content/GFPGAN/inference_gfpgan.py -i $input_path \\\n",
        "        -o $output_dir \\\n",
        "        -s 1 -v 1.3 --bg_upsampler realesrgan\n",
        "    %cd /content\n",
        "    filename = os.path.basename(input_path)\n",
        "    restored_imgs_dir = os.path.join(output_dir, \"restored_imgs\")\n",
        "    output_path = os.path.join(restored_imgs_dir, filename)\n",
        "    new_path = os.path.join(restored_imgs_dir, filename.split(\".\")[0] + \"_GFPGAN.png\")\n",
        "    os.rename(output_path, new_path)\n",
        "    return new_path\n",
        "\n",
        "\n",
        "def realesrgan_interface(input_path: str, output_dir: str, scale: int, model_name: str, fp32: bool) -> str:\n",
        "    clean_env()\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    fp32_opt = \"\"\n",
        "    if fp32:\n",
        "        fp32_opt = \"--fp32\"\n",
        "    %cd /content/Real-ESRGAN/\n",
        "    !python /content/Real-ESRGAN/inference_realesrgan.py \\\n",
        "        -i $input_path -o $output_dir -n $model_name --outscale $scale $fp32_opt\n",
        "    %cd /content\n",
        "    base_filename = os.path.basename(input_path).split(\".\")[0]\n",
        "    output_path = os.path.join(output_dir, f\"{base_filename}_out.png\")\n",
        "    new_output_path = os.path.join(output_dir, f\"{base_filename}_{scale}x.png\")\n",
        "    os.rename(output_path, new_output_path)\n",
        "    return new_output_path\n",
        "\n",
        "\n",
        "def swinir_interface(input_path: str, output_dir: str, scale: int, model_name: str) -> str:\n",
        "    %cd /content/SwinIR\n",
        "    model_path = \"/content/SwinIR/models/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.pth\"\n",
        "    large = \"\"\n",
        "    if model_name == \"SwinIR-Large\":\n",
        "        model_path = \"/content/SwinIR/models/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth\"\n",
        "        large = \"--large_model\"\n",
        "    image_dir = \"/content/SwinIR/testsets/txt2imghd\"\n",
        "    filename = os.path.basename(input_path)\n",
        "    if os.path.isdir(image_dir):\n",
        "        shutil.rmtree(image_dir)\n",
        "    os.makedirs(image_dir)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    shutil.copy(input_path, os.path.join(image_dir, filename))\n",
        "    print(\"model name \", model_name)\n",
        "    print(\"path\", model_path)\n",
        "    !python main_test_swinir.py --task real_sr --model_path $model_path --folder_lq $image_dir --scale $scale $large\n",
        "    %cd / content\n",
        "\n",
        "\n",
        "def restore_face(input_path: str, output_dir: str) -> str:\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_path = gfpgan_interface(input_path, output_dir)\n",
        "    img_before_restoration = Image.open(input_path)\n",
        "    img_after_restoration = Image.open(output_path)\n",
        "    diff, score = image_diff(img_before_restoration, img_after_restoration)\n",
        "    display_images(\n",
        "        [img_before_restoration, diff, img_after_restoration],\n",
        "        titles=[\"Before\", f\"Image Similarity: {score*100:.2f}%\", \"After\"],\n",
        "        label=\"GFPGAN\",\n",
        "    )\n",
        "    print(colored(\"Face restored\", \"green\"))\n",
        "    print(colored(output_path, \"green\"))\n",
        "    display(img_after_restoration)\n",
        "    return output_path\n",
        "\n",
        "\n",
        "def infer(prompt, strength: float, num_samples: int, steps: int, guidance_scale: float, eta: float, init_image, resize_height: Optional[int] = None, resize_width: Optional[int] = None):\n",
        "    if resize_height is not None and resize_width is not None:\n",
        "        init_image = init_image.resize(resize_height, resize_width)\n",
        "    init_image = preprocess(init_image)\n",
        "    with autocast(\"cuda\"):\n",
        "        images = pipeimg(\n",
        "            prompt,\n",
        "            init_image=init_image,\n",
        "            strength=strength,\n",
        "            guidance_scale=guidance_scale,\n",
        "            num_inference_steps=steps,\n",
        "            eta=eta\n",
        "        )[\"sample\"]\n",
        "    return images[0]\n",
        "\n",
        "\n",
        "def upscale(input: str, output_dir: str, model: str, ratio: int, scaleup_step_by_step: bool, fp32: bool, prompt: Optional[str], strength: float, steps: int, guidance_scale: float, eta: float):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    shutil.copy(input, os.path.join(output_dir, os.path.basename(input)))\n",
        "    original_image = Image.open(input)\n",
        "    og_size = original_image.size\n",
        "    current_filepath = input\n",
        "    scaleup_times = 1\n",
        "    if scaleup_step_by_step and ratio != 2:\n",
        "        if ratio == 4:\n",
        "            scaleup_times = 2\n",
        "        if ratio == 8:\n",
        "            scaleup_times = 3\n",
        "        ratio = 2\n",
        "\n",
        "    for i in range(scaleup_times):\n",
        "        if \"RealESRGAN_x4plus\" in model:\n",
        "            current_filepath = realesrgan_interface(\n",
        "                current_filepath,\n",
        "                output_dir,\n",
        "                ratio,\n",
        "                model,\n",
        "                fp32,\n",
        "            )\n",
        "        elif \"SwinIR\" in model:\n",
        "            current_filepath = swinir_interface(\n",
        "                current_filepath,\n",
        "                output_dir,\n",
        "                ratio,\n",
        "                model,\n",
        "            )\n",
        "        source_image = Image.open(current_filepath)\n",
        "        print(colored(f\"Upscaled Image with {model}\", \"green\"))\n",
        "        print(colored(current_filepath, \"green\"))\n",
        "        display(source_image)\n",
        "        if prompt is None:\n",
        "            continue\n",
        "        gobig_overlap = 128\n",
        "        slices, _ = grid_slice(source_image, gobig_overlap, og_size, False)\n",
        "        betterslices = []\n",
        "        for _, chunk_w_coords in tqdm(enumerate(slices), \"Slices\"):\n",
        "            chunk, coord_x, coord_y = chunk_w_coords\n",
        "            inferred_img = infer(\n",
        "                prompt=prompt,\n",
        "                init_image=chunk,\n",
        "                strength=strength,\n",
        "                steps=steps,\n",
        "                guidance_scale=guidance_scale,\n",
        "                eta=eta,\n",
        "                num_samples=1\n",
        "            )\n",
        "            betterslices.append((inferred_img.convert('RGBA'), coord_x, coord_y))\n",
        "\n",
        "        alpha = Image.new('L', og_size, color=0xFF)\n",
        "        alpha_gradient = ImageDraw.Draw(alpha)\n",
        "        a = 0\n",
        "        i = 0\n",
        "        shape = (og_size, (0, 0))\n",
        "        while i < gobig_overlap:\n",
        "            alpha_gradient.rectangle(shape, fill=a)\n",
        "            a += 4\n",
        "            i += 1\n",
        "            shape = ((og_size[0] - i, og_size[1] - i), (i, i))\n",
        "        mask = Image.new('RGBA', og_size, color=0)\n",
        "        mask.putalpha(alpha)\n",
        "        finished_slices = []\n",
        "        for betterslice, x, y in betterslices:\n",
        "            finished_slice = addalpha(betterslice, mask)\n",
        "            finished_slices.append((finished_slice, x, y))\n",
        "        final_output = grid_merge(source_image.convert(\"RGBA\"), finished_slices).convert(\"RGB\")\n",
        "        current_filename = os.path.basename(current_filepath).split(\".\")[0]\n",
        "        current_filepath = os.path.join(output_dir, f\"{current_filename}_img2img.png\")\n",
        "        final_output.save(current_filepath)\n",
        "        diff, score = image_diff(source_image, final_output)\n",
        "        display_images(\n",
        "            [source_image, diff, final_output],\n",
        "            titles=[\"Before\", f\"Image Similarity: {score*100:.2f}%\", \"After\"],\n",
        "            label=\"Real-ESRGAN with img2img\",\n",
        "        )\n",
        "        print(colored(f\"Img2Img (steps: {steps}, strength: {strength})\", \"green\"))\n",
        "        print(colored(current_filepath, \"green\"))\n",
        "        display(final_output)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    return current_filepath\n",
        "\n",
        "\n",
        "def text_2_img():\n",
        "    clean_env()\n",
        "    if GFPGAN_BEFORE_SCALEUP or GFPGAN_AFTER_SCALEUP:\n",
        "        install_gfpgan()\n",
        "\n",
        "    if SCALEUP_RATIO != \"1x\":\n",
        "        install_esrgan()\n",
        "\n",
        "    setup_dirs(ROOT_DIR)\n",
        "    original_dir = os.path.join(ROOT_DIR, \"original\")\n",
        "    os.makedirs(original_dir, exist_ok=True)\n",
        "\n",
        "    for i in range(IMG_NUM):\n",
        "        clean_env()\n",
        "        now = dt.datetime.now()\n",
        "        filename = now.strftime(\"%Y%m%d-%H%M%S\") + \".png\"\n",
        "        filepath = os.path.join(original_dir, filename)\n",
        "        if BASEIMAGE_PATH == \"\":\n",
        "            img = generate_image(PROMPT, WIDTH, HEIGHT, SEED, STEPS, TXT2IMG_GUIDANCE_SCALE, TXT2IMG_ETA)\n",
        "            print(colored(\"Image by txt2img\", \"green\"))\n",
        "        else:\n",
        "            img = Image.open(BASEIMAGE_PATH)\n",
        "            print(colored(f\"Existing image({BASEIMAGE_PATH})\", \"green\"))\n",
        "        img.save(filepath)\n",
        "        print(colored(filepath, \"green\"))\n",
        "        display(img)\n",
        "        if GFPGAN_BEFORE_SCALEUP:\n",
        "            restored_dir = os.path.join(ROOT_DIR, \"restored\")\n",
        "            filepath = restore_face(filepath, restored_dir)\n",
        "            img = Image.open(filepath)\n",
        "        if SCALEUP_RATIO != \"1x\":\n",
        "            upscaled_dir = os.path.join(ROOT_DIR, \"upscaled\")\n",
        "            prompt = PROMPT if USE_IMG2IMG else None\n",
        "            filepath = upscale(\n",
        "                input=filepath,\n",
        "                output_dir=upscaled_dir,\n",
        "                ratio=int(SCALEUP_RATIO[0]),\n",
        "                scaleup_step_by_step=SCALEUP_STEP_BY_STEP,\n",
        "                model=ESRGAN_MODEL.split(\" \")[0],\n",
        "                prompt=prompt,\n",
        "                steps=IMG2IMG_STEPS,\n",
        "                strength=IMG2IMG_STRENGTH,\n",
        "                guidance_scale=IMG2IMG_GUIDANCE_SCALE,\n",
        "                eta=IMG2IMG_ETA,\n",
        "                fp32=FP_32\n",
        "            )\n",
        "            img = Image.open(filepath)\n",
        "        if GFPGAN_AFTER_SCALEUP:\n",
        "            restored_dir = os.path.join(ROOT_DIR, \"restored\")\n",
        "            filepath = restore_face(filepath, restored_dir)\n",
        "            img = Image.open(filepath)\n",
        "        result_dir = os.path.join(ROOT_DIR, \"results\")\n",
        "        os.makedirs(result_dir, exist_ok=True)\n",
        "        result_path = os.path.join(result_dir, os.path.basename(filepath))\n",
        "        img.save(result_path)\n",
        "\n",
        "        print(colored(f\"Result in {result_path}\", \"green\"))\n",
        "\n",
        "        clean_env()\n",
        "\n",
        "\n",
        "text_2_img()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN4PFsnGF9pMPESEmEoYWIc",
      "collapsed_sections": [
        "yJ1TOlU1O-HB"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "a0c7ee3e3feddf670ca159667eab2945657ec994c831a3bbd519f257280033d2"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e0689f0c293475ea274322bfd297916": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2823a646a40d49e2bdf497b9ef44265b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c92cfd62ea040c99bb495f7a4cc4d51",
            "placeholder": "​",
            "style": "IPY_MODEL_6acc436290854a14a28043af278fd420",
            "value": " 125/? [00:42&lt;00:00,  2.91it/s]"
          }
        },
        "328027f965204ede97a457cee1fd3125": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f754fc966e94676b4e267341f3738f9",
            "placeholder": "​",
            "style": "IPY_MODEL_d13e56a8614f46f79bcbaaf781bd2a0a",
            "value": " 36/? [05:01&lt;00:00,  8.36s/it]"
          }
        },
        "3ae5ef36ecfb4e8daab17ef28ad1b82d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3f754fc966e94676b4e267341f3738f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fd714eb5c664d92a3c55ffc6938382a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4c92cfd62ea040c99bb495f7a4cc4d51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5322167985a540668ab686f642f44f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_823bf02fa2a5431b8454e36b44ecc7b5",
            "placeholder": "​",
            "style": "IPY_MODEL_723fb6a799bc4394bf62f391ab36852d",
            "value": "Slices: "
          }
        },
        "5cad527c54e74619b178dc6a9c79d2e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aeca7972e03349a9b56ca814e8c3152d",
              "IPY_MODEL_bddeab8f0ed64bdfa0a0512a054dbb52",
              "IPY_MODEL_2823a646a40d49e2bdf497b9ef44265b"
            ],
            "layout": "IPY_MODEL_91b3149705e84c7d8bac8f32ba7e20e1"
          }
        },
        "5f29e970719f40aa9a90a35f55bb2823": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6acc436290854a14a28043af278fd420": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d9fd63516934c5d97e2d477acaee1fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70873c6bdd7c40ab8e09840c464d993a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "723fb6a799bc4394bf62f391ab36852d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fe666469ff5423da95e4f37eea84e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fd714eb5c664d92a3c55ffc6938382a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e0689f0c293475ea274322bfd297916",
            "value": 1
          }
        },
        "823bf02fa2a5431b8454e36b44ecc7b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9168ee9f091244dda321e42a40d9314d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91b3149705e84c7d8bac8f32ba7e20e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e200d10263a450d888f764392964a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b62c423dddf74ae6981536091e77dba6",
              "IPY_MODEL_db37bb6f066e48a89050329e1435046f",
              "IPY_MODEL_bd81897fffcf4ecf943bba885ddd9edc"
            ],
            "layout": "IPY_MODEL_ca7ea2f275824674bbf0c688e2fad8a8"
          }
        },
        "a55a784b7c05482a8047163e77bbd575": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5322167985a540668ab686f642f44f32",
              "IPY_MODEL_7fe666469ff5423da95e4f37eea84e30",
              "IPY_MODEL_328027f965204ede97a457cee1fd3125"
            ],
            "layout": "IPY_MODEL_9168ee9f091244dda321e42a40d9314d"
          }
        },
        "aeca7972e03349a9b56ca814e8c3152d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f85da414de6c46968e59b4e42a733af3",
            "placeholder": "​",
            "style": "IPY_MODEL_dcc07a758758449fbc17c2fb5a30ea1c",
            "value": ""
          }
        },
        "b62c423dddf74ae6981536091e77dba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70873c6bdd7c40ab8e09840c464d993a",
            "placeholder": "​",
            "style": "IPY_MODEL_6d9fd63516934c5d97e2d477acaee1fe",
            "value": "Slices: "
          }
        },
        "bd81897fffcf4ecf943bba885ddd9edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e70f5f4d352b473b83af69b8f9dbeec0",
            "placeholder": "​",
            "style": "IPY_MODEL_f951ea83b8f74412974b05c4a0903c56",
            "value": " 9/? [01:15&lt;00:00,  8.36s/it]"
          }
        },
        "bddeab8f0ed64bdfa0a0512a054dbb52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daa8a6ee759648b086258a1319eacfdd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f29e970719f40aa9a90a35f55bb2823",
            "value": 1
          }
        },
        "ca7ea2f275824674bbf0c688e2fad8a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d13e56a8614f46f79bcbaaf781bd2a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daa8a6ee759648b086258a1319eacfdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "db37bb6f066e48a89050329e1435046f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ae5ef36ecfb4e8daab17ef28ad1b82d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f32a7b551f724da8b2182abc515ef2ff",
            "value": 1
          }
        },
        "dcc07a758758449fbc17c2fb5a30ea1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e70f5f4d352b473b83af69b8f9dbeec0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f32a7b551f724da8b2182abc515ef2ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f85da414de6c46968e59b4e42a733af3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f951ea83b8f74412974b05c4a0903c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
